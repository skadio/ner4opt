{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111a5991",
   "metadata": {},
   "source": [
    "# Augmentation: Upsampling via Duplication of in-frequent patterns\n",
    "\n",
    "\n",
    "Use infrequent patterns for data augmentation:\n",
    "\n",
    "* __OBJ_DIR__ is a verb (maximize, minimize) frequently. But there are a couple of examples, where OBJ_DIR is an adjective (__I want the cost to be minimal__). Duplicating such examples might help.\n",
    "\n",
    "* __LIMIT__ and __PARAM__ are some form of numbers . There is skew in their types. Also, __ordinals__ have a higher chance of being miss tagged by a gold NER system. So, duplicating ordinal examples might also help.\n",
    "\n",
    "* __VAR__ is mostly conjuncting noun chunks. But properly identifying a span of chunk, is difficult. Conjuncting prepositional phrases is also an infrequent pattern. __He does commercials with famous actors and commercials with regular actors__. Duplicating such examples might help.\n",
    "\n",
    "* __OBJ_NAME__: OBJ_DIR followed by a prepositional phrase, OBJ_DIR followed by a prepositional phrase followed by another prepositional phrase is a rare pattern. Duplicating such examples might help. (__maximize the number of action figures__; __minimize the number of batches of cookies__)\n",
    "\n",
    "The __suffix__ number at the end of each filename represents __how many times the pattern has been duplicated__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4925972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "datetime_object = datetime.datetime.now()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "\n",
    "root_directory = Path(os.path.abspath('')).parent\n",
    "\n",
    "with open(\n",
    "        root_directory / 'data/train.json'\n",
    ") as fname:\n",
    "    data = json.load(fname)\n",
    "\n",
    "entities_list = [\"CONST_DIR\", \"LIMIT\", \"VAR\", \"PARAM\", \"OBJ_NAME\", \"OBJ_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9f14c",
   "metadata": {},
   "source": [
    "# Looks for Patterns and their Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_map_train = {}\n",
    "train_data = data[0]['paragraphs']\n",
    "spacy_sentence_map = {}\n",
    "\n",
    "for item in tqdm(train_data):\n",
    "    \n",
    "    entities = item['entities']\n",
    "    raw_string = \" \".join(\n",
    "        [token['orth'] for token in item['sentences'][0]['tokens']])\n",
    "    spacy_sentence = nlp(raw_string)\n",
    "    \n",
    "    spacy_sentence_map[raw_string] = spacy_sentence\n",
    "    \n",
    "    pos_tags = [tok.pos_ for tok in spacy_sentence]\n",
    "    dep_tags = [tok.dep_ for tok in spacy_sentence]\n",
    "    patterns = [pos_ + \"_\" + dep_ for pos_, dep_ in zip(pos_tags, dep_tags)]\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity[-1] in pattern_map_train:\n",
    "            entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "            pattern_map_train[entity[-1]] = pattern_map_train[entity[-1]] + [(raw_string[entity[0]:entity[1]], ' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1]))]\n",
    "        else:\n",
    "            entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "            pattern_map_train[entity[-1]] = [(raw_string[entity[0]:entity[1]],' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1]))]\n",
    "\n",
    "final_pattern_count_dict = {}\n",
    "\n",
    "for key in entities_list:\n",
    "    key_phrases = [val[0] for val in pattern_map_train[key]]\n",
    "    patterns_pos_dep = [val[1] for val in pattern_map_train[key]]\n",
    "    \n",
    "    pattern_counter = Counter(patterns_pos_dep)\n",
    "    \n",
    "    phrase_values = []\n",
    "    for element in pattern_counter:\n",
    "        phrase_values.append(list(set([key_phrases[val_index] for val_index, val in enumerate(patterns_pos_dep) if element == val])))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(pattern_counter, orient='index').reset_index()\n",
    "    df = df.rename(columns={'index': 'Pattern', 0: 'count'})\n",
    "    df['examples'] = phrase_values\n",
    "    final_df = df.sort_values(by=['count'], ascending=True)\n",
    "    \n",
    "    count_values = df[\"count\"].tolist()\n",
    "    patterns_final = df[\"Pattern\"].tolist()\n",
    "    \n",
    "    final_pattern_count_dict[key] = {item:count_values[indx] for indx, item in enumerate(patterns_final)}\n",
    "\n",
    "print(\"Patterns in VAR Class: \", final_pattern_count_dict['VAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9534e11",
   "metadata": {},
   "source": [
    "# Duplicate in-frequent Patterns using number_of_duplications needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_duplications = 5\n",
    "\n",
    "augmented_train = []\n",
    "\n",
    "for item in tqdm(train_data):\n",
    "    entities = item['entities']\n",
    "    \n",
    "    raw_string = \" \".join(\n",
    "        [token['orth'] for token in item['sentences'][0]['tokens']])\n",
    "    spacy_sentence = spacy_sentence_map[raw_string]\n",
    "    \n",
    "    pos_tags = [tok.pos_ for tok in spacy_sentence]\n",
    "    dep_tags = [tok.dep_ for tok in spacy_sentence]\n",
    "    patterns = [pos_ + \"_\" + dep_ for pos_, dep_ in zip(pos_tags, dep_tags)]\n",
    "    \n",
    "    number_of_replications = 0\n",
    "    for entity in entities:\n",
    "        entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "        pattern = ' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1])\n",
    "        label = entity[-1]\n",
    "        if final_pattern_count_dict[label][pattern] < number_of_duplications:\n",
    "            difference = number_of_duplications - final_pattern_count_dict[label][pattern]\n",
    "            number_of_replications = max(number_of_replications, difference)\n",
    "    \n",
    "    if number_of_replications > 0:\n",
    "        augmented_train += [item] * number_of_replications\n",
    "    else:\n",
    "        augmented_train += [item]\n",
    "\n",
    "augmented_train = augmented_train[1::]\n",
    "\n",
    "augmented_train_data = []\n",
    "\n",
    "random.shuffle(augmented_train)\n",
    "\n",
    "# spacy to regular iob\n",
    "for example in tqdm(augmented_train):\n",
    "    for token in example['sentences'][0]['tokens']:\n",
    "        word = token['orth']\n",
    "        label = token['ner']\n",
    "        if \"L-\" in label:\n",
    "            label = label.replace(\"L-\", \"I-\")\n",
    "        elif \"U-\" in label:\n",
    "            label = label.replace(\"U-\", \"B-\")\n",
    "        word_list = [word, '_', '_', label]\n",
    "        augmented_train_data.append('\\t'.join(word_list))\n",
    "    augmented_train_data.append('\\n')\n",
    "\n",
    "# add back the docstart before writing the file\n",
    "augmented_train_data = ['-DOCSTART-\\t_\\t_\\tO', '\\n'] + augmented_train_data\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "fname = root_directory / 'data' / ('augmented_train' + str(number_of_duplications) + \"__\" + str(datetime_object) + '.txt')\n",
    "\n",
    "with open(fname, 'w') as f:\n",
    "    for line in augmented_train_data:\n",
    "        if line == '\\n':\n",
    "            f.write(f\"{line}\")\n",
    "        else:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984f70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
