{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111a5991",
   "metadata": {},
   "source": [
    "# Augmentation: Upsampling via Duplication of in-frequent patterns\n",
    "\n",
    "\n",
    "Use infrequent patterns for data augmentation:\n",
    "\n",
    "* __OBJ_DIR__ is a verb (maximize, minimize) frequently. But there are a couple of examples, where OBJ_DIR is an adjective (__I want the cost to be minimal__). Duplicating such examples might help.\n",
    "\n",
    "* __LIMIT__ and __PARAM__ are some form of numbers . There is skew in their types. Also, __ordinals__ have a higher chance of being miss tagged by a gold NER system. So, duplicating ordinal examples might also help.\n",
    "\n",
    "* __VAR__ is mostly conjuncting noun chunks. But properly identifying a span of chunk, is difficult. Conjuncting prepositional phrases is also an infrequent pattern. __He does commercials with famous actors and commercials with regular actors__. Duplicating such examples might help.\n",
    "\n",
    "* __OBJ_NAME__: OBJ_DIR followed by a prepositional phrase, OBJ_DIR followed by a prepositional phrase followed by another prepositional phrase is a rare pattern. Duplicating such examples might help. (__maximize the number of action figures__; __minimize the number of batches of cookies__)\n",
    "\n",
    "The __suffix__ number at the end of each filename represents __how many times the pattern has been duplicated__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cbc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependancies\n",
    "\n",
    "# !pip install spacy==3.3.0  # version lower because of allennlp\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4925972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "datetime_object = datetime.datetime.now()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "\n",
    "root_directory = Path(os.path.abspath('')).parents[1]\n",
    "\n",
    "with open(\n",
    "        root_directory / 'spacy_format_data/train.json'\n",
    ") as fname:\n",
    "    data = json.load(fname)\n",
    "\n",
    "entities_list = [\"CONST_DIR\", \"LIMIT\", \"VAR\", \"PARAM\", \"OBJ_NAME\", \"OBJ_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9f14c",
   "metadata": {},
   "source": [
    "# Looks for Patterns and their Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43d4948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                    | 0/714 [00:00<?, ?it/s]/Users/a715824/NL4Opt%20Dummy%20Submission/test_env/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 714/714 [03:10<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns in VAR Class:  {'NOUN_compound NOUN_compound NOUN_pobj': 57, 'ADJ_amod NOUN_pobj': 279, 'NOUN_compound NOUN_compound NOUN_appos': 3, 'ADJ_amod NOUN_conj': 100, 'NOUN_compound NOUN_compound': 47, 'ADJ_amod NOUN_compound NOUN_pobj': 63, 'ADJ_amod NOUN_compound NOUN_nsubj': 30, 'PROPN_attr': 1, 'PROPN_compound PROPN_conj': 14, 'PROPN_compound': 3, 'PROPN_compound PROPN_compound': 5, 'NOUN_compound NOUN_appos': 46, 'NOUN_compound NOUN_conj': 161, 'NOUN_compound NOUN_nsubjpass': 20, 'NOUN_compound NOUN_dobj': 124, 'NOUN_conj': 294, 'NOUN_compound NOUN_pobj': 531, 'NOUN_pobj': 654, 'NOUN_dobj': 235, 'PROPN_appos': 21, 'PROPN_conj': 25, 'PROPN_pobj': 35, 'ADJ_compound NOUN_dobj': 2, 'NOUN_compound NOUN_attr': 15, 'NOUN_compound NOUN_nsubj': 203, 'NOUN_dobj PROPN_appos': 4, 'VERB_amod NOUN_appos': 2, 'VERB_amod NOUN_pobj': 17, 'VERB_amod NOUN_conj': 8, 'VERB_amod NOUN_dobj': 10, 'PROPN_compound PROPN_appos': 10, 'PROPN_compound PROPN_nsubj': 43, 'NOUN_nsubj': 293, 'NOUN_compound NOUN_compound NOUN_nsubj': 42, 'NOUN_compound PUNCT_punct NOUN_dobj': 2, 'NOUN_compound PUNCT_punct NOUN_nsubj': 6, 'NOUN_compound PUNCT_punct NOUN_pobj': 7, 'PROPN_amod NOUN_pobj': 1, 'ADJ_amod NOUN_nsubj': 137, 'ADJ_compound NOUN_nsubj': 2, 'NOUN_appos': 41, 'ADJ_amod NOUN_compound NOUN_appos': 5, 'ADJ_amod NOUN_compound NOUN_conj': 15, 'ADJ_amod NOUN_compound NOUN_dobj': 13, 'PROPN_compound NOUN_pobj': 42, 'PROPN_compound PROPN_pobj': 26, 'ADJ_amod': 53, 'ADJ_conj': 25, 'NOUN_nmod NOUN_nmod': 6, 'NOUN_compound NOUN_conj NOUN_dobj': 3, 'NOUN_ROOT ADP_prep ADJ_amod NOUN_pobj': 1, 'NOUN_conj ADP_prep ADJ_amod NOUN_pobj': 1, 'NOUN_conj ADP_prep DET_det NOUN_pobj': 1, 'NOUN_nsubj ADP_prep DET_det ADJ_amod NOUN_pobj': 1, 'NOUN_nsubj ADP_prep ADJ_amod NOUN_pobj': 1, 'NOUN_nsubj ADP_prep DET_det NOUN_pobj': 1, 'NOUN_dobj ADP_prep ADJ_amod NOUN_pobj': 1, 'NOUN_attr ADP_prep DET_det NOUN_pobj': 1, 'NOUN_attr ADP_prep ADJ_amod NOUN_pobj': 1, 'VERB_amod NOUN_nsubj': 12, 'PROPN_oprd': 2, 'PROPN_compound NOUN_nsubj': 18, 'NOUN_conj NOUN_appos': 3, 'NOUN_compound NOUN_conj NOUN_compound NOUN_dobj': 1, 'NOUN_compound NOUN_compound NOUN_compound NOUN_pobj': 10, 'NOUN_compound PROPN_appos': 1, 'NOUN_compound PROPN_punct': 1, 'NOUN_compound PROPN_pobj': 15, 'NOUN_compound NOUN_compound NOUN_dobj': 11, 'NOUN_compound NOUN_compound NOUN_conj': 17, 'ADJ_conj NOUN_appos': 3, 'NOUN_nmod': 52, 'ADJ_amod NOUN_nsubjpass': 5, 'NOUN_attr': 28, 'ADJ_amod NOUN_appos': 24, 'ADJ_amod NOUN_compound': 34, 'ADJ_amod NOUN_dobj': 94, 'NOUN_npadvmod VERB_amod NOUN_conj': 3, 'PROPN_compound PROPN_compound NOUN_pobj': 7, 'NOUN_compound PROPN_compound NOUN_pobj': 1, 'ADJ_conj NOUN_dobj': 11, 'ADJ_appos': 3, 'ADJ_punct': 1, 'ADJ_pobj': 2, 'PROPN_compound PROPN_compound PROPN_nsubj': 2, 'PROPN_compound PROPN_compound PROPN_pobj': 3, 'NOUN_conj NOUN_dobj': 30, 'NOUN_conj NOUN_pobj': 18, 'NOUN_amod NOUN_pobj': 4, 'NOUN_compound NOUN_punct': 2, 'NOUN_conj NOUN_compound NOUN_compound': 2, 'NOUN_npadvmod PUNCT_punct VERB_conj': 1, 'NOUN_npadvmod PUNCT_punct VERB_conj NOUN_pobj': 1, 'NOUN_npadvmod PUNCT_punct VERB_amod NOUN_pobj': 2, 'NOUN_compound PUNCT_punct NOUN_appos': 1, 'ADJ_amod PUNCT_punct NOUN_conj': 1, 'ADJ_amod PUNCT_punct NOUN_nsubj': 1, 'ADJ_amod PUNCT_punct NOUN_pobj': 2, 'NOUN_npadvmod': 2, 'NOUN_ROOT NUM_nummod': 1, 'NOUN_conj NUM_nummod': 1, 'NOUN_pobj NUM_nummod': 16, 'NOUN_nsubjpass': 35, 'NOUN_compound NOUN_compound PROPN_pobj': 4, 'PROPN_compound NOUN_appos': 5, 'PROPN_compound NOUN_conj': 7, 'ADJ_conj NOUN_compound': 2, 'NUM_nummod NOUN_pobj': 3, 'NUM_nummod NOUN_compound NOUN_nsubj': 3, 'NOUN_npadvmod VERB_acomp': 2, 'NOUN_npadvmod VERB_amod NOUN_pobj': 2, 'ADJ_amod NOUN_attr': 9, 'PROPN_nmod': 1, 'NOUN_compound PROPN_nsubj': 9, 'NOUN_compound PROPN_dobj': 3, 'NOUN_dobj PROPN_punct': 1, 'ADJ_conj NOUN_compound NOUN_dobj': 6, 'NOUN_poss PART_case NOUN_conj': 3, 'NOUN_poss PART_case NOUN_pobj': 12, 'ADJ_amod NOUN_compound NOUN_attr': 3, 'NOUN_compound': 11, 'NOUN_compound PUNCT_punct NOUN_conj': 2, 'NOUN_compound PUNCT_punct NOUN_nsubjpass': 3, 'PROPN_nsubj NUM_nummod': 4, 'ADJ_conj NOUN_pobj': 4, 'NOUN_dobj NUM_nummod': 6, 'NOUN_compound PROPN_oprd NUM_nummod': 1, 'NOUN_compound NOUN_conj NUM_nummod': 1, 'NOUN_compound NOUN_pobj NUM_nummod': 4, 'NOUN_nsubj NUM_nummod': 10, 'ADJ_amod NOUN_compound PUNCT_punct NOUN_nsubj': 4, 'ADJ_amod NOUN_compound PUNCT_punct NOUN_pobj': 4, 'NOUN_pobj PRON_appos': 1, 'PROPN_compound PROPN_compound NOUN_nsubj': 2, 'NOUN_poss PART_case NOUN_compound NOUN_conj': 1, 'NOUN_poss PART_case NOUN_compound NOUN_nsubj': 1, 'NOUN_poss PART_case NOUN_compound NOUN_pobj': 1, 'ADJ_amod CCONJ_cc': 1, 'NOUN_compound NOUN_compound NOUN_compound NOUN_dobj': 4, 'NOUN_compound NOUN_dep': 1, 'ADJ_amod ADJ_conj NOUN_compound PUNCT_punct NOUN_dobj': 1, 'ADV_advmod ADJ_amod NOUN_compound PUNCT_punct NOUN_nsubj': 1, 'ADV_advmod ADJ_amod NOUN_compound PUNCT_punct NOUN_pobj': 1, 'ADJ_amod ADJ_amod PUNCT_punct NOUN_appos': 1, 'ADJ_amod ADJ_amod PUNCT_punct NOUN_conj': 2, 'ADJ_amod ADJ_amod PUNCT_punct NOUN_dobj': 1, 'ADJ_amod ADJ_amod PUNCT_punct NOUN_pobj': 1, 'NOUN_amod ADJ_amod PUNCT_punct NOUN_conj': 1, 'NOUN_compound PUNCT_punct NOUN_compound NOUN_conj': 3, 'NOUN_compound PUNCT_punct NOUN_compound NOUN_pobj': 5, 'ADV_advmod VERB_amod NOUN_conj': 1, 'ADV_advmod VERB_amod NOUN_pobj': 1, 'PROPN_poss PART_case NOUN_appos': 1, 'PROPN_poss PART_case NOUN_conj': 1, 'PROPN_poss PART_case NOUN_pobj': 5, 'VERB_relcl': 4, 'ADJ_amod ADJ_amod NOUN_dobj': 3, 'ADJ_amod ADJ_amod NOUN_pobj': 2, 'NOUN_conj PUNCT_punct': 3, 'NOUN_pobj PUNCT_punct': 1, 'NUM_nummod PUNCT_punct NOUN_nmod': 1, 'NUM_nummod PUNCT_punct NOUN_conj': 1, 'NUM_nummod PUNCT_punct NOUN_compound NOUN_dobj': 2, 'NUM_nummod PUNCT_punct NOUN_compound NOUN_conj': 2, 'NUM_nummod PUNCT_punct NOUN_compound NOUN_pobj': 2, 'ADJ_amod ADJ_amod NOUN_conj': 3, 'ADJ_amod NOUN_poss': 6, 'ADJ_acomp': 5, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_conj': 5, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_attr': 3, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_pobj': 8, 'ADJ_amod NOUN_ROOT': 1, 'NOUN_amod VERB_amod NOUN_conj': 1, 'VERB_amod NOUN_compound': 3, 'VERB_amod NOUN_attr': 1, 'NOUN_nmod CCONJ_cc NOUN_conj': 5, 'NOUN_attr CCONJ_cc NOUN_conj': 1, 'VERB_amod PUNCT_punct ADP_prt': 1, 'VERB_amod PUNCT_punct ADP_prt NOUN_appos': 1, 'VERB_amod PUNCT_punct ADP_prt NOUN_nsubj': 1, 'PROPN_compound PROPN_oprd': 1, 'NUM_compound PROPN_conj': 1, 'NUM_compound PROPN_compound': 1, 'PROPN_compound PROPN_compound NOUN_dobj': 1, 'NOUN_conj NOUN_compound NOUN_dobj': 1, 'NOUN_punct': 1, 'PROPN_compound PROPN_ROOT': 1, 'NOUN_compound NOUN_compound NOUN_punct': 1, 'NOUN_compound NOUN_compound NOUN_compound': 1, 'ADJ_amod NOUN_compound NOUN_compound': 3, 'NOUN_poss PART_case NOUN_nsubj': 1, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_compound': 3, 'NOUN_npadvmod PUNCT_punct ADJ_amod NOUN_compound': 2, 'NOUN_npadvmod PUNCT_punct ADJ_amod NOUN_nsubj': 1, 'NOUN_npadvmod PUNCT_punct ADJ_amod NOUN_pobj': 2, 'PROPN_nsubj': 4, 'NOUN_compound NOUN_compound NOUN_compound NOUN_conj': 2, 'ADJ_amod NOUN_compound NOUN_dep': 1, 'NOUN_compound NOUN_compound NOUN_dep': 1, 'NOUN_nmod ADJ_amod NOUN_dobj': 1, 'NOUN_amod ADJ_amod': 1, 'NOUN_compound ADJ_compound NOUN_pobj': 1, 'NOUN_compound ADJ_amod': 1, 'ADJ_amod PUNCT_punct NOUN_compound': 1, 'NOUN_compound PUNCT_punct NOUN_compound': 1, 'ADJ_amod PUNCT_punct NOUN_attr': 2, 'PROPN_compound NOUN_attr': 4, 'VERB_amod NOUN_compound NOUN_dobj': 2, 'VERB_amod NOUN_compound NOUN_pobj': 2, 'PROPN_compound NOUN_dobj': 4, 'PROPN_compound NOUN_compound': 1, 'NOUN_compound PUNCT_punct NOUN_compound NOUN_dobj': 2, 'ADP_conj NOUN_pobj': 1, 'ADV_advmod PUNCT_punct ADJ_amod NOUN_attr': 2, 'ADV_advmod PUNCT_punct ADJ_amod NOUN_pobj': 2, 'ADJ_nmod': 1, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_nsubj': 3, 'ADJ_amod NOUN_nmod': 4, 'NOUN_npadvmod PUNCT_punct ADJ_amod NOUN_conj': 3, 'NOUN_dep': 2, 'NOUN_nmod CCONJ_cc NOUN_conj NOUN_pobj': 1, 'NOUN_dep CCONJ_cc NOUN_conj': 1, 'NUM_nummod NOUN_dobj': 1, 'NUM_nummod NOUN_compound NOUN_pobj': 2, 'NUM_nummod NOUN_compound NOUN_attr': 1, 'PROPN_nummod NOUN_dobj': 1, 'NUM_nummod NOUN_nsubjpass': 1, 'ADJ_compound NOUN_compound NOUN_nsubj': 1, 'NOUN_pobj PROPN_appos': 1, 'NOUN_pobj NUM_npadvmod': 1, 'NOUN_amod NOUN_conj': 1, 'ADJ_amod PUNCT_punct NOUN_compound NOUN_dobj': 2, 'NOUN_pobj NOUN_dobj': 1, 'ADJ_amod NOUN_compound NOUN_nsubjpass': 1, 'PROPN_dobj': 1, 'NOUN_conj NOUN_compound NOUN_compound NOUN_dobj': 1, 'NOUN_compound NUM_appos': 1, 'NOUN_amod NUM_conj': 1, 'PROPN_compound PROPN_dobj': 1, 'PROPN_compound PROPN_compound PROPN_conj': 1, 'NOUN_npadvmod PUNCT_punct VERB_amod NOUN_compound NOUN_conj': 1, 'NOUN_npadvmod PUNCT_punct VERB_amod NOUN_compound NOUN_dobj': 1, 'NOUN_amod PUNCT_punct NOUN_compound NOUN_pobj': 1, 'ADJ_compound NOUN_pobj': 1, 'ADJ_amod NOUN_conj NOUN_pobj': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern_map_train = {}\n",
    "train_data = data[0]['paragraphs']\n",
    "spacy_sentence_map = {}\n",
    "\n",
    "for item in tqdm(train_data):\n",
    "    \n",
    "    entities = item['entities']\n",
    "    raw_string = \" \".join(\n",
    "        [token['orth'] for token in item['sentences'][0]['tokens']])\n",
    "    spacy_sentence = nlp(raw_string)\n",
    "    \n",
    "    spacy_sentence_map[raw_string] = spacy_sentence\n",
    "    \n",
    "    pos_tags = [tok.pos_ for tok in spacy_sentence]\n",
    "    dep_tags = [tok.dep_ for tok in spacy_sentence]\n",
    "    patterns = [pos_ + \"_\" + dep_ for pos_, dep_ in zip(pos_tags, dep_tags)]\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity[-1] in pattern_map_train:\n",
    "            entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "            pattern_map_train[entity[-1]] = pattern_map_train[entity[-1]] + [(raw_string[entity[0]:entity[1]], ' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1]))]\n",
    "        else:\n",
    "            entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "            pattern_map_train[entity[-1]] = [(raw_string[entity[0]:entity[1]],' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1]))]\n",
    "\n",
    "final_pattern_count_dict = {}\n",
    "\n",
    "for key in entities_list:\n",
    "    key_phrases = [val[0] for val in pattern_map_train[key]]\n",
    "    patterns_pos_dep = [val[1] for val in pattern_map_train[key]]\n",
    "    \n",
    "    pattern_counter = Counter(patterns_pos_dep)\n",
    "    \n",
    "    phrase_values = []\n",
    "    for element in pattern_counter:\n",
    "        phrase_values.append(list(set([key_phrases[val_index] for val_index, val in enumerate(patterns_pos_dep) if element == val])))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(pattern_counter, orient='index').reset_index()\n",
    "    df = df.rename(columns={'index': 'Pattern', 0: 'count'})\n",
    "    df['examples'] = phrase_values\n",
    "    final_df = df.sort_values(by=['count'], ascending=True)\n",
    "    \n",
    "    count_values = df[\"count\"].tolist()\n",
    "    patterns_final = df[\"Pattern\"].tolist()\n",
    "    \n",
    "    final_pattern_count_dict[key] = {item:count_values[indx] for indx, item in enumerate(patterns_final)}\n",
    "\n",
    "print(\"Patterns in VAR Class: \", final_pattern_count_dict['VAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9534e11",
   "metadata": {},
   "source": [
    "# Duplicate in-frequent Patterns using number_of_duplications needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3429e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 714/714 [00:00<00:00, 7790.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1159/1159 [00:00<00:00, 16278.55it/s]\n"
     ]
    }
   ],
   "source": [
    "number_of_duplications = 5\n",
    "\n",
    "augmented_train = []\n",
    "\n",
    "for item in tqdm(train_data):\n",
    "    entities = item['entities']\n",
    "    \n",
    "    raw_string = \" \".join(\n",
    "        [token['orth'] for token in item['sentences'][0]['tokens']])\n",
    "    spacy_sentence = spacy_sentence_map[raw_string]\n",
    "    \n",
    "    pos_tags = [tok.pos_ for tok in spacy_sentence]\n",
    "    dep_tags = [tok.dep_ for tok in spacy_sentence]\n",
    "    patterns = [pos_ + \"_\" + dep_ for pos_, dep_ in zip(pos_tags, dep_tags)]\n",
    "    \n",
    "    number_of_replications = 0\n",
    "    for entity in entities:\n",
    "        entity_span = spacy_sentence.char_span(entity[0], entity[1])\n",
    "        pattern = ' '.join(patterns[entity_span[0].i:entity_span[-1].i + 1])\n",
    "        label = entity[-1]\n",
    "        if final_pattern_count_dict[label][pattern] < number_of_duplications:\n",
    "            difference = number_of_duplications - final_pattern_count_dict[label][pattern]\n",
    "            number_of_replications = max(number_of_replications, difference)\n",
    "    \n",
    "    if number_of_replications > 0:\n",
    "        augmented_train += [item] * number_of_replications\n",
    "    else:\n",
    "        augmented_train += [item]\n",
    "\n",
    "augmented_train = augmented_train[1::]\n",
    "\n",
    "augmented_train_data = []\n",
    "\n",
    "random.shuffle(augmented_train)\n",
    "\n",
    "# spacy to regular iob\n",
    "for example in tqdm(augmented_train):\n",
    "    for token in example['sentences'][0]['tokens']:\n",
    "        word = token['orth']\n",
    "        label = token['ner']\n",
    "        if \"L-\" in label:\n",
    "            label = label.replace(\"L-\", \"I-\")\n",
    "        elif \"U-\" in label:\n",
    "            label = label.replace(\"U-\", \"B-\")\n",
    "        word_list = [word, '_', '_', label]\n",
    "        augmented_train_data.append('\\t'.join(word_list))\n",
    "    augmented_train_data.append('\\n')\n",
    "\n",
    "# add back the docstart before writing the file\n",
    "augmented_train_data = ['-DOCSTART-\\t_\\t_\\tO', '\\n'] + augmented_train_data\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "fname = root_directory / 'augmented_train_data' / ('augmented_train' + str(number_of_duplications) + \"__\" + str(datetime_object) + '.txt')\n",
    "\n",
    "with open(fname, 'w') as f:\n",
    "    for line in augmented_train_data:\n",
    "        if line == '\\n':\n",
    "            f.write(f\"{line}\")\n",
    "        else:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
